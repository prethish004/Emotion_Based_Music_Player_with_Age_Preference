{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "import opendatasets as od\n",
        "import pandas\n",
        "od.download(\"https://www.kaggle.com/datasets/msambare/fer2013\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1BHVhQay3Lp",
        "outputId": "f3701244-ca63-4031-c0b0-b68dd5eaf22e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.10)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username:Your Kaggle Key: MANIKEY\n",
            "Dataset URL: https://www.kaggle.com/datasets/msambare/fer2013\n",
            "Downloading fer2013.zip to ./fer2013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60.3M/60.3M [00:00<00:00, 166MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F5uZKDcLwSUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AN6QTKBVwSXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import os\n",
        "\n",
        "# Set paths for dataset\n",
        "train_dir = \"/content/fer2013/train\"\n",
        "val_dir = \"/content/fer2013/test\"\n",
        "\n",
        "# Hyperparameters\n",
        "IMG_SIZE = (224, 224)  # MobileNetV2 input size\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_CLASSES = len(os.listdir(train_dir))  # Number of emotion categories\n",
        "\n",
        "# Data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "# Load pre-trained model\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "\n",
        "# Freeze base layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom layers for emotion recognition\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save(\"emotion_recognition_model.h5\")\n",
        "\n",
        "print(\"Training completed. Model saved as 'emotion_recognition_model.h5' and best model as 'best_emotion_model.h5'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_i2L6AVyrWj",
        "outputId": "936849e6-98a2-414d-9919-5c42b64cba8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Epoch 1/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 427ms/step - accuracy: 0.2110 - loss: 1.8899 - val_accuracy: 0.2669 - val_loss: 1.7550\n",
            "Epoch 2/25\n",
            "\u001b[1m  1/897\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:56\u001b[0m 130ms/step - accuracy: 0.2812 - loss: 1.8097"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.2812 - loss: 1.8097 - val_accuracy: 0.5000 - val_loss: 1.6381\n",
            "Epoch 3/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 420ms/step - accuracy: 0.2779 - loss: 1.7583 - val_accuracy: 0.2963 - val_loss: 1.7246\n",
            "Epoch 4/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79us/step - accuracy: 0.3438 - loss: 1.6428 - val_accuracy: 0.3000 - val_loss: 1.7269\n",
            "Epoch 5/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 418ms/step - accuracy: 0.2996 - loss: 1.7204 - val_accuracy: 0.3029 - val_loss: 1.7048\n",
            "Epoch 6/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.3750 - loss: 1.5665 - val_accuracy: 0.4000 - val_loss: 1.6400\n",
            "Epoch 7/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 422ms/step - accuracy: 0.3083 - loss: 1.7062 - val_accuracy: 0.3192 - val_loss: 1.6841\n",
            "Epoch 8/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.3750 - loss: 1.5927 - val_accuracy: 0.2000 - val_loss: 1.6395\n",
            "Epoch 9/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 418ms/step - accuracy: 0.3202 - loss: 1.6951 - val_accuracy: 0.3085 - val_loss: 1.6825\n",
            "Epoch 10/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.2812 - loss: 1.6777 - val_accuracy: 0.6000 - val_loss: 1.6854\n",
            "Epoch 11/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 418ms/step - accuracy: 0.3340 - loss: 1.6744 - val_accuracy: 0.3315 - val_loss: 1.6602\n",
            "Epoch 12/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84us/step - accuracy: 0.3125 - loss: 1.5250 - val_accuracy: 0.2000 - val_loss: 1.5586\n",
            "Epoch 13/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 417ms/step - accuracy: 0.3379 - loss: 1.6607 - val_accuracy: 0.3259 - val_loss: 1.6692\n",
            "Epoch 14/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78us/step - accuracy: 0.3125 - loss: 1.8069 - val_accuracy: 0.3000 - val_loss: 1.6450\n",
            "Epoch 15/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 417ms/step - accuracy: 0.3447 - loss: 1.6547 - val_accuracy: 0.3341 - val_loss: 1.6466\n",
            "Epoch 16/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.3750 - loss: 1.5059 - val_accuracy: 0.3000 - val_loss: 1.6656\n",
            "Epoch 17/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 417ms/step - accuracy: 0.3463 - loss: 1.6511 - val_accuracy: 0.3446 - val_loss: 1.6405\n",
            "Epoch 18/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80us/step - accuracy: 0.3125 - loss: 1.6599 - val_accuracy: 0.2000 - val_loss: 2.0097\n",
            "Epoch 19/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 421ms/step - accuracy: 0.3528 - loss: 1.6408 - val_accuracy: 0.3262 - val_loss: 1.6598\n",
            "Epoch 20/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.1875 - loss: 1.9233 - val_accuracy: 0.2000 - val_loss: 2.0686\n",
            "Epoch 21/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 416ms/step - accuracy: 0.3528 - loss: 1.6369 - val_accuracy: 0.3238 - val_loss: 1.6529\n",
            "Epoch 22/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82us/step - accuracy: 0.4062 - loss: 1.4595 - val_accuracy: 0.3000 - val_loss: 1.5793\n",
            "Epoch 23/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 424ms/step - accuracy: 0.3554 - loss: 1.6305 - val_accuracy: 0.3361 - val_loss: 1.6389\n",
            "Epoch 24/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80us/step - accuracy: 0.4375 - loss: 1.5401 - val_accuracy: 0.4000 - val_loss: 1.5559\n",
            "Epoch 25/25\n",
            "\u001b[1m897/897\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 418ms/step - accuracy: 0.3602 - loss: 1.6191 - val_accuracy: 0.3379 - val_loss: 1.6382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed. Model saved as 'emotion_recognition_model.h5' and best model as 'best_emotion_model.h5'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Load the trained model\n",
        "model_path = \"/content/emotion_recognition_model.h5\"  # Replace with the path to your model\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Emotion classes\n",
        "class_labels = [\"angryðŸ˜ \", \"disgustðŸ¤¢\", \"fearðŸ˜±\", \"happyâ˜ºï¸\", \"neutralðŸ˜\", \"sadðŸ˜ž\", \"surpriseðŸŽŠ\"]\n",
        "\n",
        "# Preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    try:\n",
        "        img = Image.open(image_path).convert(\"RGB\")\n",
        "        img = img.resize((224, 224))  # Resize to the input size of the model\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "        img_array = img_array / 255.0  # Normalize pixel values\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test the model\n",
        "def test_model(image_path):\n",
        "    print(f\"Testing with image: {image_path}\")\n",
        "    img_array = preprocess_image(image_path)\n",
        "    if img_array is not None:\n",
        "        predictions = model.predict(img_array)\n",
        "        predicted_class = class_labels[np.argmax(predictions)]\n",
        "        confidence = np.max(predictions)\n",
        "        print(f\"Predicted Emotion: {predicted_class}\")\n",
        "        print(f\"Confidence: {confidence:.2f}\")\n",
        "    else:\n",
        "        print(\"Image preprocessing failed.\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to a test image\n",
        "    test_image_path = \"/content/5.jpg\"  # Replace with the path to your test image\n",
        "\n",
        "    if os.path.exists(test_image_path):\n",
        "        test_model(test_image_path)\n",
        "    else:\n",
        "        print(\"Test image not found. Please check the file path.\")\n"
      ],
      "metadata": {
        "id": "7BGrBvjXzCuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29880206-b8a5-4c1a-e49c-0e6587207b87"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test image not found. Please check the file path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kB5Mr61t-ntX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOolblrS32Pb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}